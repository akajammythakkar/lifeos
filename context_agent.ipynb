{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from enum import Enum\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from graphiti_core import Graphiti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple context quality assessment\n",
    "class ContextQuality(Enum):\n",
    "    INSUFFICIENT = \"insufficient\"\n",
    "    SUFFICIENT = \"sufficient\"\n",
    "    EXCESSIVE = \"excessive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleContextAgent:\n",
    "    def __init__(self):\n",
    "        # Setup\n",
    "        self.llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "        self.client = Graphiti(\n",
    "            os.environ.get('NEO4J_URI', 'bolt://localhost:7687'),\n",
    "            os.environ.get('NEO4J_USER', 'neo4j'),\n",
    "            os.environ.get('NEO4J_PASSWORD', 'password')\n",
    "        )\n",
    "        \n",
    "    async def get_context(self, query: str, max_results: int = None) -> tuple[List[str], List[Dict]]:\n",
    "        \"\"\"Get context from knowledge base and return both facts and detailed results\"\"\"\n",
    "        try:\n",
    "            # Dynamic max_results based on query if not specified\n",
    "            if max_results is None:\n",
    "                max_results = await self.determine_initial_fetch_size(query)\n",
    "            \n",
    "            print(f\"\\nğŸ” SEARCHING GRAPH DATABASE:\")\n",
    "            print(f\"   Query: '{query}'\")\n",
    "            print(f\"   Dynamic Max Results: {max_results}\")\n",
    "            \n",
    "            results = await self.client.search(query, num_results=max_results)\n",
    "            \n",
    "            # Extract facts and detailed information\n",
    "            facts = []\n",
    "            detailed_results = []\n",
    "            \n",
    "            print(f\"\\nğŸ“Š GRAPH DATABASE RESULTS ({len(results)} items found):\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for i, edge in enumerate(results, 1):\n",
    "                fact = edge.fact\n",
    "                facts.append(fact)\n",
    "                \n",
    "                # Create detailed result info\n",
    "                result_info = {\n",
    "                    'index': i,\n",
    "                    'fact': fact,\n",
    "                    'confidence': getattr(edge, 'confidence', 'N/A'),\n",
    "                    'source_node': getattr(edge, 'source_node', 'N/A'),\n",
    "                    'target_node': getattr(edge, 'target_node', 'N/A'),\n",
    "                    'edge_type': getattr(edge, 'edge_type', 'N/A'),\n",
    "                    'metadata': getattr(edge, 'metadata', {})\n",
    "                }\n",
    "                detailed_results.append(result_info)\n",
    "                \n",
    "                print(f\"{i:2d}. Fact: {fact}\")\n",
    "                print(f\"    Confidence: {result_info['confidence']}\")\n",
    "                print(f\"    Source: {result_info['source_node']}\")\n",
    "                print(f\"    Target: {result_info['target_node']}\")\n",
    "                print(f\"    Type: {result_info['edge_type']}\")\n",
    "                if result_info['metadata']:\n",
    "                    print(f\"    Metadata: {result_info['metadata']}\")\n",
    "                print()\n",
    "            \n",
    "            if not results:\n",
    "                print(\"   âŒ No results found in graph database\")\n",
    "            \n",
    "            return facts, detailed_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error searching graph database: {str(e)}\")\n",
    "            return [], []\n",
    "    \n",
    "    async def determine_initial_fetch_size(self, query: str) -> int:\n",
    "        \"\"\"Dynamically determine how many facts to fetch initially based on query complexity\"\"\"\n",
    "        \n",
    "        analysis_prompt = f\"\"\"\n",
    "        Analyze this query and determine how many facts should be initially fetched from a knowledge base:\n",
    "        \n",
    "        Query: \"{query}\"\n",
    "        \n",
    "        Consider:\n",
    "        - Simple factual questions: 3-5 facts\n",
    "        - Complex multi-part questions: 8-12 facts  \n",
    "        - Broad exploratory questions: 10-15 facts\n",
    "        - Comparison questions: 6-10 facts\n",
    "        - Technical/detailed questions: 8-12 facts\n",
    "        \n",
    "        Respond with ONLY a number between 3 and 15.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"   ğŸ§  Analyzing query complexity...\")\n",
    "            response = await self.llm.ainvoke([HumanMessage(content=analysis_prompt)])\n",
    "            initial_size = int(response.content.strip())\n",
    "            \n",
    "            # Clamp to reasonable bounds\n",
    "            initial_size = max(3, min(15, initial_size))\n",
    "            print(f\"   ğŸ“Š Determined initial fetch size: {initial_size}\")\n",
    "            return initial_size\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error determining fetch size: {str(e)}\")\n",
    "            print(f\"   ğŸ”„ Using default size: 8\")\n",
    "            return 8  # Default fallback\n",
    "    \n",
    "    async def assess_context_quality(self, query: str, context: List[str], target_answer_depth: str = \"auto\") -> Dict[str, Any]:\n",
    "        \"\"\"Check if context is sufficient, insufficient, or excessive\"\"\"\n",
    "        \n",
    "        # Determine target depth if auto\n",
    "        if target_answer_depth == \"auto\":\n",
    "            target_answer_depth = await self.determine_answer_depth_needed(query)\n",
    "        \n",
    "        print(f\"\\nğŸ” ASSESSING CONTEXT QUALITY:\")\n",
    "        print(f\"   Query: '{query}'\")\n",
    "        print(f\"   Context items: {len(context)}\")\n",
    "        print(f\"   Target answer depth: {target_answer_depth}\")\n",
    "        \n",
    "        assessment_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "        Target Answer Depth: {target_answer_depth}\n",
    "        \n",
    "        Context Facts ({len(context)} items):\n",
    "        {chr(10).join([f\"- {fact}\" for fact in context]) if context else \"No context provided\"}\n",
    "        \n",
    "        Assess if this context is sufficient for a {target_answer_depth} answer to this query.\n",
    "        \n",
    "        Respond in JSON:\n",
    "        {{\n",
    "            \"quality\": \"insufficient|sufficient|excessive\",\n",
    "            \"confidence\": 0.0-1.0,\n",
    "            \"reasoning\": \"brief explanation\",\n",
    "            \"can_answer\": true/false,\n",
    "            \"missing_info\": \"what key information is missing (if any)\",\n",
    "            \"relevance_score\": 0.0-1.0,\n",
    "            \"recommended_context_size\": number_of_facts_needed,\n",
    "            \"current_coverage\": 0.0-1.0\n",
    "        }}\n",
    "        \n",
    "        Guidelines based on target depth:\n",
    "        - brief: 2-4 highly relevant facts sufficient\n",
    "        - moderate: 4-8 relevant facts needed  \n",
    "        - comprehensive: 8-15+ facts for thorough coverage\n",
    "        - insufficient: Missing key information needed\n",
    "        - sufficient: Has enough relevant information for target depth\n",
    "        - excessive: Too much irrelevant information that clutters\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"   ğŸ“¤ Sending assessment prompt to LLM...\")\n",
    "            response = await self.llm.ainvoke([HumanMessage(content=assessment_prompt)])\n",
    "            assessment = json.loads(response.content)\n",
    "            \n",
    "            print(f\"   ğŸ“¥ LLM Assessment Result:\")\n",
    "            print(f\"      Quality: {assessment.get('quality', 'unknown')}\")\n",
    "            print(f\"      Confidence: {assessment.get('confidence', 0):.2f}\")\n",
    "            print(f\"      Can Answer: {assessment.get('can_answer', False)}\")\n",
    "            print(f\"      Reasoning: {assessment.get('reasoning', 'N/A')}\")\n",
    "            print(f\"      Missing Info: {assessment.get('missing_info', 'N/A')}\")\n",
    "            print(f\"      Relevance Score: {assessment.get('relevance_score', 0):.2f}\")\n",
    "            print(f\"      Recommended Size: {assessment.get('recommended_context_size', 'N/A')}\")\n",
    "            print(f\"      Current Coverage: {assessment.get('current_coverage', 0):.2f}\")\n",
    "            \n",
    "            return assessment\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error in LLM assessment: {str(e)}\")\n",
    "            # Fallback simple assessment\n",
    "            fallback = {\n",
    "                \"quality\": \"insufficient\" if not context else (\"excessive\" if len(context) > 12 else \"sufficient\"),\n",
    "                \"confidence\": 1.0 if not context else (0.7 if len(context) > 12 else 0.8),\n",
    "                \"reasoning\": \"No context\" if not context else (\"Too many facts\" if len(context) > 12 else \"Reasonable amount\"),\n",
    "                \"can_answer\": bool(context),\n",
    "                \"missing_info\": \"No context available\" if not context else \"N/A\",\n",
    "                \"relevance_score\": 0.0 if not context else 0.7,\n",
    "                \"recommended_context_size\": 8 if not context else len(context),\n",
    "                \"current_coverage\": 0.0 if not context else 0.7\n",
    "            }\n",
    "            print(f\"   ğŸ”„ Using fallback assessment: {fallback}\")\n",
    "            return fallback\n",
    "    \n",
    "    async def determine_answer_depth_needed(self, query: str) -> str:\n",
    "        \"\"\"Determine if query needs brief, moderate, or comprehensive answer\"\"\"\n",
    "        \n",
    "        depth_prompt = f\"\"\"\n",
    "        Analyze this query and determine what depth of answer is needed:\n",
    "        \n",
    "        Query: \"{query}\"\n",
    "        \n",
    "        Choose ONE:\n",
    "        - brief: Simple factual answer, 1-2 sentences\n",
    "        - moderate: Explanation with some detail, 1-2 paragraphs  \n",
    "        - comprehensive: Detailed analysis with examples, multiple paragraphs\n",
    "        \n",
    "        Respond with ONLY the word: brief, moderate, or comprehensive\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = await self.llm.ainvoke([HumanMessage(content=depth_prompt)])\n",
    "            depth = response.content.strip().lower()\n",
    "            if depth in ['brief', 'moderate', 'comprehensive']:\n",
    "                return depth\n",
    "            return 'moderate'  # Default\n",
    "        except:\n",
    "            return 'moderate'  # Default fallback\n",
    "    \n",
    "    async def optimize_context(self, query: str, initial_context: List[str], detailed_results: List[Dict]) -> tuple[List[str], List[Dict]]:\n",
    "        \"\"\"Get the right amount of context - not too much, not too little\"\"\"\n",
    "        \n",
    "        print(f\"\\nğŸ”§ OPTIMIZING CONTEXT:\")\n",
    "        print(f\"   Initial context items: {len(initial_context)}\")\n",
    "        \n",
    "        if not initial_context:\n",
    "            print(\"   ğŸ“ˆ No initial context - trying broader search...\")\n",
    "            # Dynamic broader search based on assessment\n",
    "            broader_size = max(15, len(initial_context) * 2)\n",
    "            broader_context, broader_details = await self.get_context(query, max_results=broader_size)\n",
    "            \n",
    "            # Use smart selection instead of just first N\n",
    "            optimized_context, optimized_details = await self.smart_select_context(\n",
    "                query, broader_context, broader_details, target_size=\"auto\"\n",
    "            )\n",
    "            print(f\"   âœ… Broader search returned {len(broader_context)} items, intelligently selected {len(optimized_context)}\")\n",
    "            return optimized_context, optimized_details\n",
    "        \n",
    "        # Smart context selection\n",
    "        print(\"   ğŸ¯ Using intelligent context selection...\")\n",
    "        return await self.smart_select_context(query, initial_context, detailed_results, target_size=\"auto\")\n",
    "    \n",
    "    async def smart_select_context(self, query: str, context: List[str], details: List[Dict], target_size: str = \"auto\") -> tuple[List[str], List[Dict]]:\n",
    "        \"\"\"Intelligently select the optimal number and combination of context facts\"\"\"\n",
    "        \n",
    "        if target_size == \"auto\":\n",
    "            # Determine optimal size based on query and available context\n",
    "            target_size = await self.determine_optimal_context_size(query, len(context))\n",
    "        \n",
    "        print(f\"   ğŸ¯ Smart selecting from {len(context)} facts, target: {target_size}\")\n",
    "        \n",
    "        if len(context) <= target_size:\n",
    "            print(f\"   âœ… Available context ({len(context)}) <= target ({target_size}), using all\")\n",
    "            return context, details\n",
    "        \n",
    "        # Use LLM to intelligently select best facts\n",
    "        selection_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "        Target number of facts: {target_size}\n",
    "        \n",
    "        Available Context:\n",
    "        {chr(10).join([f\"{i+1}. {fact}\" for i, fact in enumerate(context)])}\n",
    "        \n",
    "        Select the {target_size} most relevant and complementary facts that together provide the best coverage for answering this query.\n",
    "        \n",
    "        Consider:\n",
    "        - Direct relevance to the query\n",
    "        - Complementary information (avoid redundancy)  \n",
    "        - Comprehensive coverage of the topic\n",
    "        - Quality and specificity of information\n",
    "        \n",
    "        Return ONLY the numbers separated by commas (e.g., \"1,3,5,7,9\"):\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"   ğŸ“¤ Sending smart selection prompt to LLM...\")\n",
    "            response = await self.llm.ainvoke([HumanMessage(content=selection_prompt)])\n",
    "            selected_nums = [int(n.strip()) for n in response.content.split(',') if n.strip().isdigit()]\n",
    "            \n",
    "            # Validate selection\n",
    "            selected_nums = [n for n in selected_nums if 0 < n <= len(context)]\n",
    "            if not selected_nums:\n",
    "                # Fallback to first target_size items\n",
    "                selected_nums = list(range(1, min(target_size + 1, len(context) + 1)))\n",
    "            \n",
    "            print(f\"   ğŸ“¥ LLM selected indices: {selected_nums}\")\n",
    "            \n",
    "            # Apply selection\n",
    "            selected_context = [context[i-1] for i in selected_nums]\n",
    "            selected_details = [details[i-1] for i in selected_nums]\n",
    "            \n",
    "            print(f\"   âœ… Selected {len(selected_context)} optimal facts\")\n",
    "            \n",
    "            # Print selected items\n",
    "            for i, fact in enumerate(selected_context, 1):\n",
    "                print(f\"   {i}. Selected: {fact[:80]}...\")\n",
    "            \n",
    "            return selected_context, selected_details\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error in smart selection: {str(e)}\")\n",
    "            print(f\"   ğŸ”„ Using fallback: first {target_size} items\")\n",
    "            return context[:target_size], details[:target_size]\n",
    "    \n",
    "    async def determine_optimal_context_size(self, query: str, available_count: int) -> int:\n",
    "        \"\"\"Determine optimal number of context facts needed for this specific query\"\"\"\n",
    "        \n",
    "        optimization_prompt = f\"\"\"\n",
    "        Query: \"{query}\"\n",
    "        Available facts: {available_count}\n",
    "        \n",
    "        Determine the optimal number of facts needed to answer this query well.\n",
    "        \n",
    "        Consider:\n",
    "        - Query complexity and scope\n",
    "        - Need for comprehensive vs focused answer\n",
    "        - Risk of information overload vs insufficient detail\n",
    "        \n",
    "        Respond with ONLY a number between 2 and {min(available_count, 15)}.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = await self.llm.ainvoke([HumanMessage(content=optimization_prompt)])\n",
    "            optimal_size = int(response.content.strip())\n",
    "            \n",
    "            # Clamp to reasonable bounds\n",
    "            optimal_size = max(2, min(available_count, optimal_size))\n",
    "            print(f\"   ğŸ“Š Optimal context size determined: {optimal_size}\")\n",
    "            return optimal_size\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error determining optimal size: {str(e)}\")\n",
    "            # Smart fallback based on available count\n",
    "            fallback_size = min(8, max(3, available_count // 2))\n",
    "            print(f\"   ğŸ”„ Using smart fallback size: {fallback_size}\")\n",
    "            return fallback_size\n",
    "    \n",
    "    async def answer_with_context(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main method: Get context, assess quality, and answer\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ğŸš€ PROCESSING QUERY: '{query}'\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Step 1: Get initial context\n",
    "        print(f\"\\nğŸ“ STEP 1: INITIAL CONTEXT RETRIEVAL\")\n",
    "        initial_context, initial_details = await self.get_context(query)\n",
    "        \n",
    "        # Step 2: Assess context quality\n",
    "        print(f\"\\nğŸ“ STEP 2: INITIAL CONTEXT ASSESSMENT\")\n",
    "        assessment = await self.assess_context_quality(query, initial_context)\n",
    "        \n",
    "        # Step 3: Optimize context if needed\n",
    "        print(f\"\\nğŸ“ STEP 3: DYNAMIC CONTEXT OPTIMIZATION\")\n",
    "        if assessment[\"quality\"] == \"insufficient\":\n",
    "            print(\"   ğŸ”„ Context insufficient - expanding search dynamically...\")\n",
    "            \n",
    "            # Determine how much more context we need\n",
    "            recommended_size = assessment.get(\"recommended_context_size\", len(initial_context) * 2)\n",
    "            expanded_context, expanded_details = await self.get_context(query, max_results=recommended_size)\n",
    "            \n",
    "            optimized_context, optimized_details = await self.smart_select_context(\n",
    "                query, expanded_context, expanded_details, target_size=\"auto\"\n",
    "            )\n",
    "        elif assessment[\"quality\"] == \"excessive\":\n",
    "            print(\"   ğŸ”„ Context excessive - intelligently reducing...\")\n",
    "            \n",
    "            # Determine optimal reduced size\n",
    "            target_size = max(3, assessment.get(\"recommended_context_size\", len(initial_context) // 2))\n",
    "            optimized_context, optimized_details = await self.smart_select_context(\n",
    "                query, initial_context, initial_details, target_size=target_size\n",
    "            )\n",
    "        else:\n",
    "            print(\"   âœ… Context quality sufficient - minor optimization...\")\n",
    "            # Even for sufficient context, do light optimization\n",
    "            optimized_context, optimized_details = await self.smart_select_context(\n",
    "                query, initial_context, initial_details, target_size=\"auto\"\n",
    "            )\n",
    "        \n",
    "        # Step 4: Final assessment\n",
    "        print(f\"\\nğŸ“ STEP 4: FINAL CONTEXT ASSESSMENT\")\n",
    "        final_assessment = await self.assess_context_quality(query, optimized_context)\n",
    "        \n",
    "        # Step 5: Generate answer\n",
    "        print(f\"\\nğŸ“ STEP 5: ANSWER GENERATION\")\n",
    "        if final_assessment[\"can_answer\"]:\n",
    "            print(\"   âœ… Sufficient context available - generating answer...\")\n",
    "            \n",
    "            # Print final context being used\n",
    "            print(f\"\\nğŸ“‹ FINAL CONTEXT BEING USED FOR ANSWER:\")\n",
    "            for i, (fact, detail) in enumerate(zip(optimized_context, optimized_details), 1):\n",
    "                print(f\"   {i}. {fact}\")\n",
    "                print(f\"      â””â”€ Confidence: {detail.get('confidence', 'N/A')}\")\n",
    "                print(f\"      â””â”€ Source: {detail.get('source_node', 'N/A')} â†’ {detail.get('target_node', 'N/A')}\")\n",
    "            \n",
    "            answer_prompt = f\"\"\"\n",
    "            Query: {query}\n",
    "            \n",
    "            Context:\n",
    "            {chr(10).join([f\"- {fact}\" for fact in optimized_context])}\n",
    "            \n",
    "            Provide a helpful answer based on the context above. Be specific about which facts you're using.\n",
    "            \"\"\"\n",
    "            \n",
    "            print(\"   ğŸ“¤ Sending answer generation prompt to LLM...\")\n",
    "            answer_response = await self.llm.ainvoke([HumanMessage(content=answer_prompt)])\n",
    "            answer = answer_response.content\n",
    "            print(f\"   ğŸ“¥ Answer generated ({len(answer)} characters)\")\n",
    "        else:\n",
    "            print(\"   âŒ Insufficient context - cannot provide reliable answer\")\n",
    "            answer = \"I don't have enough relevant information to answer your question properly.\"\n",
    "        \n",
    "        # Compile final result\n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"initial_context_count\": len(initial_context),\n",
    "            \"final_context_count\": len(optimized_context),\n",
    "            \"context_quality\": final_assessment[\"quality\"],\n",
    "            \"confidence\": final_assessment[\"confidence\"],\n",
    "            \"reasoning\": final_assessment[\"reasoning\"],\n",
    "            \"context_facts\": optimized_context,\n",
    "            \"detailed_context\": optimized_details,\n",
    "            \"initial_assessment\": assessment,\n",
    "            \"final_assessment\": final_assessment,\n",
    "            \"answer\": answer\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nğŸ‰ PROCESSING COMPLETE\")\n",
    "        print(f\"   Final Context Quality: {result['context_quality']}\")\n",
    "        print(f\"   Confidence: {result['confidence']:.2f}\")\n",
    "        print(f\"   Context Items Used: {result['final_context_count']}\")\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool for external use\n",
    "@tool\n",
    "async def smart_search(query: str) -> str:\n",
    "    \"\"\"Search with intelligent context optimization\"\"\"\n",
    "    agent = SimpleContextAgent()\n",
    "    result = await agent.answer_with_context(query)\n",
    "    \n",
    "    return f\"\"\"\n",
    "    Context Quality: {result['context_quality']} (confidence: {result['confidence']:.2f})\n",
    "    Context Used: {result['final_context_count']} facts\n",
    "    Reasoning: {result['reasoning']}\n",
    "    \n",
    "    Answer: {result['answer']}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸš€ PROCESSING QUERY: 'What are the best running shoes?'\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ STEP 1: INITIAL CONTEXT RETRIEVAL\n",
      "   ğŸ§  Analyzing query complexity...\n",
      "   ğŸ“Š Determined initial fetch size: 10\n",
      "\n",
      "ğŸ” SEARCHING GRAPH DATABASE:\n",
      "   Query: 'What are the best running shoes?'\n",
      "   Dynamic Max Results: 10\n",
      "\n",
      "ğŸ“Š GRAPH DATABASE RESULTS (10 items found):\n",
      "------------------------------------------------------------\n",
      " 1. Fact: Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole) is a type of Shoes.\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 2. Fact: Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole) is a type of Shoes.\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 3. Fact: TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) is a type of Shoes.\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 4. Fact: TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) is a type of Shoes.\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 5. Fact: TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size variant 7T.\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 6. Fact: TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size variant 10T.\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 7. Fact: TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size variant 8T.\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 8. Fact: TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size variant 9T.\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 9. Fact: Manybirds is the vendor of Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole).\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      "10. Fact: TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size variant 6T.\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      "\n",
      "ğŸ“ STEP 2: INITIAL CONTEXT ASSESSMENT\n",
      "\n",
      "ğŸ” ASSESSING CONTEXT QUALITY:\n",
      "   Query: 'What are the best running shoes?'\n",
      "   Context items: 10\n",
      "   Target answer depth: moderate\n",
      "   ğŸ“¤ Sending assessment prompt to LLM...\n",
      "   âŒ Error in LLM assessment: Expecting value: line 1 column 1 (char 0)\n",
      "   ğŸ”„ Using fallback assessment: {'quality': 'sufficient', 'confidence': 0.8, 'reasoning': 'Reasonable amount', 'can_answer': True, 'missing_info': 'N/A', 'relevance_score': 0.7, 'recommended_context_size': 10, 'current_coverage': 0.7}\n",
      "\n",
      "ğŸ“ STEP 3: DYNAMIC CONTEXT OPTIMIZATION\n",
      "   âœ… Context quality sufficient - minor optimization...\n",
      "   ğŸ“Š Optimal context size determined: 6\n",
      "   ğŸ¯ Smart selecting from 10 facts, target: 6\n",
      "   ğŸ“¤ Sending smart selection prompt to LLM...\n",
      "   ğŸ“¥ LLM selected indices: [1, 3, 5, 6, 9, 10]\n",
      "   âœ… Selected 6 optimal facts\n",
      "   1. Selected: Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole) is a type of Shoes....\n",
      "   2. Selected: TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) is a type o...\n",
      "   3. Selected: TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size ...\n",
      "   4. Selected: TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size ...\n",
      "   5. Selected: Manybirds is the vendor of Men's SuperLight Wool Runners - Dark Grey (Medium Gre...\n",
      "   6. Selected: TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size ...\n",
      "\n",
      "ğŸ“ STEP 4: FINAL CONTEXT ASSESSMENT\n",
      "\n",
      "ğŸ” ASSESSING CONTEXT QUALITY:\n",
      "   Query: 'What are the best running shoes?'\n",
      "   Context items: 6\n",
      "   Target answer depth: moderate\n",
      "   ğŸ“¤ Sending assessment prompt to LLM...\n",
      "   âŒ Error in LLM assessment: Expecting value: line 1 column 1 (char 0)\n",
      "   ğŸ”„ Using fallback assessment: {'quality': 'sufficient', 'confidence': 0.8, 'reasoning': 'Reasonable amount', 'can_answer': True, 'missing_info': 'N/A', 'relevance_score': 0.7, 'recommended_context_size': 6, 'current_coverage': 0.7}\n",
      "\n",
      "ğŸ“ STEP 5: ANSWER GENERATION\n",
      "   âœ… Sufficient context available - generating answer...\n",
      "\n",
      "ğŸ“‹ FINAL CONTEXT BEING USED FOR ANSWER:\n",
      "   1. Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole) is a type of Shoes.\n",
      "      â””â”€ Confidence: N/A\n",
      "      â””â”€ Source: N/A â†’ N/A\n",
      "   2. TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) is a type of Shoes.\n",
      "      â””â”€ Confidence: N/A\n",
      "      â””â”€ Source: N/A â†’ N/A\n",
      "   3. TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size variant 7T.\n",
      "      â””â”€ Confidence: N/A\n",
      "      â””â”€ Source: N/A â†’ N/A\n",
      "   4. TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size variant 10T.\n",
      "      â””â”€ Confidence: N/A\n",
      "      â””â”€ Source: N/A â†’ N/A\n",
      "   5. Manybirds is the vendor of Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole).\n",
      "      â””â”€ Confidence: N/A\n",
      "      â””â”€ Source: N/A â†’ N/A\n",
      "   6. TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size variant 6T.\n",
      "      â””â”€ Confidence: N/A\n",
      "      â””â”€ Source: N/A â†’ N/A\n",
      "   ğŸ“¤ Sending answer generation prompt to LLM...\n",
      "   ğŸ“¥ Answer generated (872 characters)\n",
      "\n",
      "ğŸ‰ PROCESSING COMPLETE\n",
      "   Final Context Quality: sufficient\n",
      "   Confidence: 0.80\n",
      "   Context Items Used: 6\n",
      "\n",
      "================================ğŸ¯ FINAL SUMMARY=================================\n",
      "Query: What are the best running shoes?\n",
      "Initial Context: 10 facts\n",
      "Final Context: 6 facts\n",
      "Quality: sufficient\n",
      "Confidence: 0.80\n",
      "Reasoning: Reasonable amount\n",
      "\n",
      "Detailed Context Used:\n",
      "  â€¢ Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole) is a type of Shoes....\n",
      "  â€¢ TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) is a type of Shoes....\n",
      "  â€¢ TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size variant 7T....\n",
      "  â€¢ TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size variant 10T....\n",
      "  â€¢ Manybirds is the vendor of Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole)....\n",
      "  â€¢ TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size variant 6T....\n",
      "\n",
      "Final Answer:\n",
      "Based on the context provided, there are two specific types of running shoes mentioned:\n",
      "\n",
      "1. **Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole)**: This shoe is designed for men and is produced by the vendor Manybirds. It is likely to be lightweight and suitable for running due to its wool material, which can provide comfort and breathability.\n",
      "\n",
      "2. **TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole)**: This shoe is designed for little kids and comes in multiple size variants (6T, 7T, and 10T). It is made from wool, which can also offer comfort and warmth for young runners.\n",
      "\n",
      "If you are looking for running shoes, the **Men's SuperLight Wool Runners** would be a great option for adults, while the **TinyBirds Wool Runners** are suitable for children. Both options emphasize comfort and are made from wool, which is beneficial for running.\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ PROCESSING QUERY: 'Tell me about customer service policies'\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ STEP 1: INITIAL CONTEXT RETRIEVAL\n",
      "   ğŸ§  Analyzing query complexity...\n",
      "   ğŸ“Š Determined initial fetch size: 10\n",
      "\n",
      "ğŸ” SEARCHING GRAPH DATABASE:\n",
      "   Query: 'Tell me about customer service policies'\n",
      "   Dynamic Max Results: 10\n",
      "\n",
      "ğŸ“Š GRAPH DATABASE RESULTS (0 items found):\n",
      "------------------------------------------------------------\n",
      "   âŒ No results found in graph database\n",
      "\n",
      "ğŸ“ STEP 2: INITIAL CONTEXT ASSESSMENT\n",
      "\n",
      "ğŸ” ASSESSING CONTEXT QUALITY:\n",
      "   Query: 'Tell me about customer service policies'\n",
      "   Context items: 0\n",
      "   Target answer depth: moderate\n",
      "   ğŸ“¤ Sending assessment prompt to LLM...\n",
      "   âŒ Error in LLM assessment: Expecting value: line 1 column 1 (char 0)\n",
      "   ğŸ”„ Using fallback assessment: {'quality': 'insufficient', 'confidence': 1.0, 'reasoning': 'No context', 'can_answer': False, 'missing_info': 'No context available', 'relevance_score': 0.0, 'recommended_context_size': 8, 'current_coverage': 0.0}\n",
      "\n",
      "ğŸ“ STEP 3: DYNAMIC CONTEXT OPTIMIZATION\n",
      "   ğŸ”„ Context insufficient - expanding search dynamically...\n",
      "\n",
      "ğŸ” SEARCHING GRAPH DATABASE:\n",
      "   Query: 'Tell me about customer service policies'\n",
      "   Dynamic Max Results: 8\n",
      "\n",
      "ğŸ“Š GRAPH DATABASE RESULTS (0 items found):\n",
      "------------------------------------------------------------\n",
      "   âŒ No results found in graph database\n",
      "   ğŸ“Š Optimal context size determined: 2\n",
      "   ğŸ¯ Smart selecting from 0 facts, target: 2\n",
      "   âœ… Available context (0) <= target (2), using all\n",
      "\n",
      "ğŸ“ STEP 4: FINAL CONTEXT ASSESSMENT\n",
      "\n",
      "ğŸ” ASSESSING CONTEXT QUALITY:\n",
      "   Query: 'Tell me about customer service policies'\n",
      "   Context items: 0\n",
      "   Target answer depth: moderate\n",
      "   ğŸ“¤ Sending assessment prompt to LLM...\n",
      "   âŒ Error in LLM assessment: Expecting value: line 1 column 1 (char 0)\n",
      "   ğŸ”„ Using fallback assessment: {'quality': 'insufficient', 'confidence': 1.0, 'reasoning': 'No context', 'can_answer': False, 'missing_info': 'No context available', 'relevance_score': 0.0, 'recommended_context_size': 8, 'current_coverage': 0.0}\n",
      "\n",
      "ğŸ“ STEP 5: ANSWER GENERATION\n",
      "   âŒ Insufficient context - cannot provide reliable answer\n",
      "\n",
      "ğŸ‰ PROCESSING COMPLETE\n",
      "   Final Context Quality: insufficient\n",
      "   Confidence: 1.00\n",
      "   Context Items Used: 0\n",
      "\n",
      "================================ğŸ¯ FINAL SUMMARY=================================\n",
      "Query: Tell me about customer service policies\n",
      "Initial Context: 0 facts\n",
      "Final Context: 0 facts\n",
      "Quality: insufficient\n",
      "Confidence: 1.00\n",
      "Reasoning: No context\n",
      "\n",
      "Detailed Context Used:\n",
      "\n",
      "Final Answer:\n",
      "I don't have enough relevant information to answer your question properly.\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ PROCESSING QUERY: 'How do I return a product?'\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ STEP 1: INITIAL CONTEXT RETRIEVAL\n",
      "   ğŸ§  Analyzing query complexity...\n",
      "   ğŸ“Š Determined initial fetch size: 5\n",
      "\n",
      "ğŸ” SEARCHING GRAPH DATABASE:\n",
      "   Query: 'How do I return a product?'\n",
      "   Dynamic Max Results: 5\n",
      "\n",
      "ğŸ“Š GRAPH DATABASE RESULTS (5 items found):\n",
      "------------------------------------------------------------\n",
      " 1. Fact: Manybirds is the vendor of Men's Couriers - Natural Black/Basin Blue (Blizzard Sole).\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 2. Fact: jess is interested in buying a pair of shoes\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 3. Fact: Size 14 is available for Men's Couriers - Natural Black/Basin Blue (Blizzard Sole).\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 4. Fact: Size 12 is available for Men's Couriers - Natural Black/Basin Blue (Blizzard Sole).\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 5. Fact: jess asks what sizes the TinyBirds Wool Runners in Natural Black come in\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      "\n",
      "ğŸ“ STEP 2: INITIAL CONTEXT ASSESSMENT\n",
      "\n",
      "ğŸ” ASSESSING CONTEXT QUALITY:\n",
      "   Query: 'How do I return a product?'\n",
      "   Context items: 5\n",
      "   Target answer depth: brief\n",
      "   ğŸ“¤ Sending assessment prompt to LLM...\n",
      "   âŒ Error in LLM assessment: Expecting value: line 1 column 1 (char 0)\n",
      "   ğŸ”„ Using fallback assessment: {'quality': 'sufficient', 'confidence': 0.8, 'reasoning': 'Reasonable amount', 'can_answer': True, 'missing_info': 'N/A', 'relevance_score': 0.7, 'recommended_context_size': 5, 'current_coverage': 0.7}\n",
      "\n",
      "ğŸ“ STEP 3: DYNAMIC CONTEXT OPTIMIZATION\n",
      "   âœ… Context quality sufficient - minor optimization...\n",
      "   ğŸ“Š Optimal context size determined: 3\n",
      "   ğŸ¯ Smart selecting from 5 facts, target: 3\n",
      "   ğŸ“¤ Sending smart selection prompt to LLM...\n",
      "   ğŸ“¥ LLM selected indices: [1, 2, 5]\n",
      "   âœ… Selected 3 optimal facts\n",
      "   1. Selected: Manybirds is the vendor of Men's Couriers - Natural Black/Basin Blue (Blizzard S...\n",
      "   2. Selected: jess is interested in buying a pair of shoes...\n",
      "   3. Selected: jess asks what sizes the TinyBirds Wool Runners in Natural Black come in...\n",
      "\n",
      "ğŸ“ STEP 4: FINAL CONTEXT ASSESSMENT\n",
      "\n",
      "ğŸ” ASSESSING CONTEXT QUALITY:\n",
      "   Query: 'How do I return a product?'\n",
      "   Context items: 3\n",
      "   Target answer depth: brief\n",
      "   ğŸ“¤ Sending assessment prompt to LLM...\n",
      "   âŒ Error in LLM assessment: Expecting value: line 1 column 1 (char 0)\n",
      "   ğŸ”„ Using fallback assessment: {'quality': 'sufficient', 'confidence': 0.8, 'reasoning': 'Reasonable amount', 'can_answer': True, 'missing_info': 'N/A', 'relevance_score': 0.7, 'recommended_context_size': 3, 'current_coverage': 0.7}\n",
      "\n",
      "ğŸ“ STEP 5: ANSWER GENERATION\n",
      "   âœ… Sufficient context available - generating answer...\n",
      "\n",
      "ğŸ“‹ FINAL CONTEXT BEING USED FOR ANSWER:\n",
      "   1. Manybirds is the vendor of Men's Couriers - Natural Black/Basin Blue (Blizzard Sole).\n",
      "      â””â”€ Confidence: N/A\n",
      "      â””â”€ Source: N/A â†’ N/A\n",
      "   2. jess is interested in buying a pair of shoes\n",
      "      â””â”€ Confidence: N/A\n",
      "      â””â”€ Source: N/A â†’ N/A\n",
      "   3. jess asks what sizes the TinyBirds Wool Runners in Natural Black come in\n",
      "      â””â”€ Confidence: N/A\n",
      "      â””â”€ Source: N/A â†’ N/A\n",
      "   ğŸ“¤ Sending answer generation prompt to LLM...\n",
      "   ğŸ“¥ Answer generated (735 characters)\n",
      "\n",
      "ğŸ‰ PROCESSING COMPLETE\n",
      "   Final Context Quality: sufficient\n",
      "   Confidence: 0.80\n",
      "   Context Items Used: 3\n",
      "\n",
      "================================ğŸ¯ FINAL SUMMARY=================================\n",
      "Query: How do I return a product?\n",
      "Initial Context: 5 facts\n",
      "Final Context: 3 facts\n",
      "Quality: sufficient\n",
      "Confidence: 0.80\n",
      "Reasoning: Reasonable amount\n",
      "\n",
      "Detailed Context Used:\n",
      "  â€¢ Manybirds is the vendor of Men's Couriers - Natural Black/Basin Blue (Blizzard Sole)....\n",
      "  â€¢ jess is interested in buying a pair of shoes...\n",
      "  â€¢ jess asks what sizes the TinyBirds Wool Runners in Natural Black come in...\n",
      "\n",
      "Final Answer:\n",
      "To return a product, you typically need to follow the vendor's return policy. Since Manybirds is the vendor for the Men's Couriers - Natural Black/Basin Blue (Blizzard Sole), you should check their website or contact their customer service for specific return instructions. \n",
      "\n",
      "If you purchased the TinyBirds Wool Runners in Natural Black and wish to return them, ensure you have your order details handy, such as the order number and the reason for the return. Most vendors require items to be in their original condition and packaging for a successful return. \n",
      "\n",
      "If you need more specific information about the return process for Manybirds, I recommend visiting their official website or reaching out to their customer support directly.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "agent = SimpleContextAgent()\n",
    "\n",
    "# Test with different types of queries\n",
    "queries = [\n",
    "    \"What are the best running shoes?\",\n",
    "    \"Tell me about customer service policies\",\n",
    "    \"How do I return a product?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    result = await agent.answer_with_context(query)\n",
    "    \n",
    "    print(f\"\\n{'ğŸ¯ FINAL SUMMARY':=^80}\")\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Initial Context: {result['initial_context_count']} facts\")\n",
    "    print(f\"Final Context: {result['final_context_count']} facts\")\n",
    "    print(f\"Quality: {result['context_quality']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "    print(f\"Reasoning: {result['reasoning']}\")\n",
    "    print(f\"\\nDetailed Context Used:\")\n",
    "    for detail in result['detailed_context']:\n",
    "        print(f\"  â€¢ {detail['fact'][:100]}...\")\n",
    "    print(f\"\\nFinal Answer:\\n{result['answer']}\")\n",
    "    print(f\"{'='*80}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

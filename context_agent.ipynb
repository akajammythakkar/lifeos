{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from enum import Enum\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from graphiti_core import Graphiti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple context quality assessment\n",
    "class ContextQuality(Enum):\n",
    "    INSUFFICIENT = \"insufficient\"\n",
    "    SUFFICIENT = \"sufficient\"\n",
    "    EXCESSIVE = \"excessive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleContextAgent:\n",
    "    def __init__(self):\n",
    "        # Setup\n",
    "        self.llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "        self.client = Graphiti(\n",
    "            os.environ.get('NEO4J_URI', 'bolt://localhost:7687'),\n",
    "            os.environ.get('NEO4J_USER', 'neo4j'),\n",
    "            os.environ.get('NEO4J_PASSWORD', 'password')\n",
    "        )\n",
    "        \n",
    "    async def get_context(self, query: str, max_results: int = 5) -> tuple[List[str], List[Dict]]:\n",
    "        \"\"\"Get context from knowledge base and return both facts and detailed results\"\"\"\n",
    "        try:\n",
    "            print(f\"\\n🔍 SEARCHING GRAPH DATABASE:\")\n",
    "            print(f\"   Query: '{query}'\")\n",
    "            print(f\"   Max Results: {max_results}\")\n",
    "            \n",
    "            results = await self.client.search(query, num_results=max_results)\n",
    "            \n",
    "            # Extract facts and detailed information\n",
    "            facts = []\n",
    "            detailed_results = []\n",
    "            \n",
    "            print(f\"\\n📊 GRAPH DATABASE RESULTS ({len(results)} items found):\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            for i, edge in enumerate(results, 1):\n",
    "                fact = edge.fact\n",
    "                facts.append(fact)\n",
    "                \n",
    "                # Create detailed result info\n",
    "                result_info = {\n",
    "                    'index': i,\n",
    "                    'fact': fact,\n",
    "                    'confidence': getattr(edge, 'confidence', 'N/A'),\n",
    "                    'source_node': getattr(edge, 'source_node', 'N/A'),\n",
    "                    'target_node': getattr(edge, 'target_node', 'N/A'),\n",
    "                    'edge_type': getattr(edge, 'edge_type', 'N/A'),\n",
    "                    'metadata': getattr(edge, 'metadata', {})\n",
    "                }\n",
    "                detailed_results.append(result_info)\n",
    "                \n",
    "                print(f\"{i:2d}. Fact: {fact}\")\n",
    "                print(f\"    Confidence: {result_info['confidence']}\")\n",
    "                print(f\"    Source: {result_info['source_node']}\")\n",
    "                print(f\"    Target: {result_info['target_node']}\")\n",
    "                print(f\"    Type: {result_info['edge_type']}\")\n",
    "                if result_info['metadata']:\n",
    "                    print(f\"    Metadata: {result_info['metadata']}\")\n",
    "                print()\n",
    "            \n",
    "            if not results:\n",
    "                print(\"   ❌ No results found in graph database\")\n",
    "            \n",
    "            return facts, detailed_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error searching graph database: {str(e)}\")\n",
    "            return [], []\n",
    "    \n",
    "    async def assess_context_quality(self, query: str, context: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Check if context is sufficient, insufficient, or excessive\"\"\"\n",
    "        \n",
    "        print(f\"\\n🔍 ASSESSING CONTEXT QUALITY:\")\n",
    "        print(f\"   Query: '{query}'\")\n",
    "        print(f\"   Context items: {len(context)}\")\n",
    "        \n",
    "        assessment_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "        \n",
    "        Context Facts:\n",
    "        {chr(10).join([f\"- {fact}\" for fact in context]) if context else \"No context provided\"}\n",
    "        \n",
    "        Assess the context quality for answering this query. Respond in JSON:\n",
    "        {{\n",
    "            \"quality\": \"insufficient|sufficient|excessive\",\n",
    "            \"confidence\": 0.0-1.0,\n",
    "            \"reasoning\": \"brief explanation\",\n",
    "            \"can_answer\": true/false,\n",
    "            \"missing_info\": \"what key information is missing (if any)\",\n",
    "            \"relevance_score\": 0.0-1.0\n",
    "        }}\n",
    "        \n",
    "        Guidelines:\n",
    "        - insufficient: Missing key information needed to answer\n",
    "        - sufficient: Has enough relevant information to answer well\n",
    "        - excessive: Too much irrelevant information that clutters the response\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"   📤 Sending assessment prompt to LLM...\")\n",
    "            response = await self.llm.ainvoke([HumanMessage(content=assessment_prompt)])\n",
    "            assessment = json.loads(response.content)\n",
    "            \n",
    "            print(f\"   📥 LLM Assessment Result:\")\n",
    "            print(f\"      Quality: {assessment.get('quality', 'unknown')}\")\n",
    "            print(f\"      Confidence: {assessment.get('confidence', 0):.2f}\")\n",
    "            print(f\"      Can Answer: {assessment.get('can_answer', False)}\")\n",
    "            print(f\"      Reasoning: {assessment.get('reasoning', 'N/A')}\")\n",
    "            print(f\"      Missing Info: {assessment.get('missing_info', 'N/A')}\")\n",
    "            print(f\"      Relevance Score: {assessment.get('relevance_score', 0):.2f}\")\n",
    "            \n",
    "            return assessment\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error in LLM assessment: {str(e)}\")\n",
    "            # Fallback simple assessment\n",
    "            fallback = {\n",
    "                \"quality\": \"insufficient\" if not context else (\"excessive\" if len(context) > 10 else \"sufficient\"),\n",
    "                \"confidence\": 1.0 if not context else (0.7 if len(context) > 10 else 0.8),\n",
    "                \"reasoning\": \"No context\" if not context else (\"Too many facts\" if len(context) > 10 else \"Reasonable amount\"),\n",
    "                \"can_answer\": bool(context),\n",
    "                \"missing_info\": \"No context available\" if not context else \"N/A\",\n",
    "                \"relevance_score\": 0.0 if not context else 0.7\n",
    "            }\n",
    "            print(f\"   🔄 Using fallback assessment: {fallback}\")\n",
    "            return fallback\n",
    "    \n",
    "    async def optimize_context(self, query: str, initial_context: List[str], detailed_results: List[Dict]) -> tuple[List[str], List[Dict]]:\n",
    "        \"\"\"Get the right amount of context - not too much, not too little\"\"\"\n",
    "        \n",
    "        print(f\"\\n🔧 OPTIMIZING CONTEXT:\")\n",
    "        print(f\"   Initial context items: {len(initial_context)}\")\n",
    "        \n",
    "        if not initial_context:\n",
    "            print(\"   📈 No initial context - trying broader search...\")\n",
    "            # Try broader search\n",
    "            broader_context, broader_details = await self.get_context(query, max_results=10)\n",
    "            optimized_context = broader_context[:5]  # Cap at 5 for simplicity\n",
    "            optimized_details = broader_details[:5]\n",
    "            print(f\"   ✅ Broader search returned {len(broader_context)} items, using first 5\")\n",
    "            return optimized_context, optimized_details\n",
    "        \n",
    "        # Filter most relevant context\n",
    "        print(\"   🎯 Filtering most relevant context using LLM...\")\n",
    "        filter_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "        \n",
    "        Available Context:\n",
    "        {chr(10).join([f\"{i+1}. {fact}\" for i, fact in enumerate(initial_context)])}\n",
    "        \n",
    "        Select the 3-5 most relevant facts that directly help answer the query.\n",
    "        Consider relevance, completeness, and avoiding redundancy.\n",
    "        Return only the numbers separated by commas (e.g., \"1,3,5\"):\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"   📤 Sending filter prompt to LLM...\")\n",
    "            response = await self.llm.ainvoke([HumanMessage(content=filter_prompt)])\n",
    "            selected_nums = [int(n.strip()) for n in response.content.split(',') if n.strip().isdigit()]\n",
    "            \n",
    "            print(f\"   📥 LLM selected indices: {selected_nums}\")\n",
    "            \n",
    "            # Filter both facts and detailed results\n",
    "            optimized_context = [initial_context[i-1] for i in selected_nums if 0 < i <= len(initial_context)]\n",
    "            optimized_details = [detailed_results[i-1] for i in selected_nums if 0 < i <= len(detailed_results)]\n",
    "            \n",
    "            print(f\"   ✅ Optimized to {len(optimized_context)} most relevant items\")\n",
    "            \n",
    "            # Print selected items\n",
    "            for i, (fact, detail) in enumerate(zip(optimized_context, optimized_details), 1):\n",
    "                print(f\"   {i}. Selected: {fact[:100]}...\")\n",
    "            \n",
    "            return optimized_context, optimized_details\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error in context optimization: {str(e)}\")\n",
    "            print(\"   🔄 Using fallback: first 5 items\")\n",
    "            return initial_context[:5], detailed_results[:5]\n",
    "    \n",
    "    async def answer_with_context(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main method: Get context, assess quality, and answer\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"🚀 PROCESSING QUERY: '{query}'\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Step 1: Get initial context\n",
    "        print(f\"\\n📝 STEP 1: INITIAL CONTEXT RETRIEVAL\")\n",
    "        initial_context, initial_details = await self.get_context(query)\n",
    "        \n",
    "        # Step 2: Assess context quality\n",
    "        print(f\"\\n📝 STEP 2: INITIAL CONTEXT ASSESSMENT\")\n",
    "        assessment = await self.assess_context_quality(query, initial_context)\n",
    "        \n",
    "        # Step 3: Optimize context if needed\n",
    "        print(f\"\\n📝 STEP 3: CONTEXT OPTIMIZATION\")\n",
    "        if assessment[\"quality\"] == \"insufficient\":\n",
    "            print(\"   🔄 Context insufficient - expanding search...\")\n",
    "            expanded_context, expanded_details = await self.get_context(query, max_results=10)\n",
    "            optimized_context, optimized_details = await self.optimize_context(query, expanded_context, expanded_details)\n",
    "        elif assessment[\"quality\"] == \"excessive\":\n",
    "            print(\"   🔄 Context excessive - filtering to most relevant...\")\n",
    "            optimized_context, optimized_details = await self.optimize_context(query, initial_context, initial_details)\n",
    "        else:\n",
    "            print(\"   ✅ Context quality sufficient - using as is\")\n",
    "            optimized_context = initial_context\n",
    "            optimized_details = initial_details\n",
    "        \n",
    "        # Step 4: Final assessment\n",
    "        print(f\"\\n📝 STEP 4: FINAL CONTEXT ASSESSMENT\")\n",
    "        final_assessment = await self.assess_context_quality(query, optimized_context)\n",
    "        \n",
    "        # Step 5: Generate answer\n",
    "        print(f\"\\n📝 STEP 5: ANSWER GENERATION\")\n",
    "        if final_assessment[\"can_answer\"]:\n",
    "            print(\"   ✅ Sufficient context available - generating answer...\")\n",
    "            \n",
    "            # Print final context being used\n",
    "            print(f\"\\n📋 FINAL CONTEXT BEING USED FOR ANSWER:\")\n",
    "            for i, (fact, detail) in enumerate(zip(optimized_context, optimized_details), 1):\n",
    "                print(f\"   {i}. {fact}\")\n",
    "                print(f\"      └─ Confidence: {detail.get('confidence', 'N/A')}\")\n",
    "                print(f\"      └─ Source: {detail.get('source_node', 'N/A')} → {detail.get('target_node', 'N/A')}\")\n",
    "            \n",
    "            answer_prompt = f\"\"\"\n",
    "            Query: {query}\n",
    "            \n",
    "            Context:\n",
    "            {chr(10).join([f\"- {fact}\" for fact in optimized_context])}\n",
    "            \n",
    "            Provide a helpful answer based on the context above. Be specific about which facts you're using.\n",
    "            \"\"\"\n",
    "            \n",
    "            print(\"   📤 Sending answer generation prompt to LLM...\")\n",
    "            answer_response = await self.llm.ainvoke([HumanMessage(content=answer_prompt)])\n",
    "            answer = answer_response.content\n",
    "            print(f\"   📥 Answer generated ({len(answer)} characters)\")\n",
    "        else:\n",
    "            print(\"   ❌ Insufficient context - cannot provide reliable answer\")\n",
    "            answer = \"I don't have enough relevant information to answer your question properly.\"\n",
    "        \n",
    "        # Compile final result\n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"initial_context_count\": len(initial_context),\n",
    "            \"final_context_count\": len(optimized_context),\n",
    "            \"context_quality\": final_assessment[\"quality\"],\n",
    "            \"confidence\": final_assessment[\"confidence\"],\n",
    "            \"reasoning\": final_assessment[\"reasoning\"],\n",
    "            \"context_facts\": optimized_context,\n",
    "            \"detailed_context\": optimized_details,\n",
    "            \"initial_assessment\": assessment,\n",
    "            \"final_assessment\": final_assessment,\n",
    "            \"answer\": answer\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n🎉 PROCESSING COMPLETE\")\n",
    "        print(f\"   Final Context Quality: {result['context_quality']}\")\n",
    "        print(f\"   Confidence: {result['confidence']:.2f}\")\n",
    "        print(f\"   Context Items Used: {result['final_context_count']}\")\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool for external use\n",
    "@tool\n",
    "async def smart_search(query: str) -> str:\n",
    "    \"\"\"Search with intelligent context optimization\"\"\"\n",
    "    agent = SimpleContextAgent()\n",
    "    result = await agent.answer_with_context(query)\n",
    "    \n",
    "    return f\"\"\"\n",
    "    Context Quality: {result['context_quality']} (confidence: {result['confidence']:.2f})\n",
    "    Context Used: {result['final_context_count']} facts\n",
    "    Reasoning: {result['reasoning']}\n",
    "    \n",
    "    Answer: {result['answer']}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING QUERY: 'What are the best running shoes?'\n",
      "================================================================================\n",
      "\n",
      "📝 STEP 1: INITIAL CONTEXT RETRIEVAL\n",
      "\n",
      "🔍 SEARCHING GRAPH DATABASE:\n",
      "   Query: 'What are the best running shoes?'\n",
      "   Max Results: 5\n",
      "\n",
      "📊 GRAPH DATABASE RESULTS (5 items found):\n",
      "------------------------------------------------------------\n",
      " 1. Fact: Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole) is a type of Shoes.\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 2. Fact: Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole) is a type of Shoes.\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 3. Fact: TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) is a type of Shoes.\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 4. Fact: TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) is a type of Shoes.\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 5. Fact: TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size variant 7T.\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      "\n",
      "📝 STEP 2: INITIAL CONTEXT ASSESSMENT\n",
      "\n",
      "🔍 ASSESSING CONTEXT QUALITY:\n",
      "   Query: 'What are the best running shoes?'\n",
      "   Context items: 5\n",
      "   📤 Sending assessment prompt to LLM...\n",
      "   ❌ Error in LLM assessment: Expecting value: line 1 column 1 (char 0)\n",
      "   🔄 Using fallback assessment: {'quality': 'sufficient', 'confidence': 0.8, 'reasoning': 'Reasonable amount', 'can_answer': True, 'missing_info': 'N/A', 'relevance_score': 0.7}\n",
      "\n",
      "📝 STEP 3: CONTEXT OPTIMIZATION\n",
      "   ✅ Context quality sufficient - using as is\n",
      "\n",
      "📝 STEP 4: FINAL CONTEXT ASSESSMENT\n",
      "\n",
      "🔍 ASSESSING CONTEXT QUALITY:\n",
      "   Query: 'What are the best running shoes?'\n",
      "   Context items: 5\n",
      "   📤 Sending assessment prompt to LLM...\n",
      "   ❌ Error in LLM assessment: Expecting value: line 1 column 1 (char 0)\n",
      "   🔄 Using fallback assessment: {'quality': 'sufficient', 'confidence': 0.8, 'reasoning': 'Reasonable amount', 'can_answer': True, 'missing_info': 'N/A', 'relevance_score': 0.7}\n",
      "\n",
      "📝 STEP 5: ANSWER GENERATION\n",
      "   ✅ Sufficient context available - generating answer...\n",
      "\n",
      "📋 FINAL CONTEXT BEING USED FOR ANSWER:\n",
      "   1. Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole) is a type of Shoes.\n",
      "      └─ Confidence: N/A\n",
      "      └─ Source: N/A → N/A\n",
      "   2. Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole) is a type of Shoes.\n",
      "      └─ Confidence: N/A\n",
      "      └─ Source: N/A → N/A\n",
      "   3. TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) is a type of Shoes.\n",
      "      └─ Confidence: N/A\n",
      "      └─ Source: N/A → N/A\n",
      "   4. TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) is a type of Shoes.\n",
      "      └─ Confidence: N/A\n",
      "      └─ Source: N/A → N/A\n",
      "   5. TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size variant 7T.\n",
      "      └─ Confidence: N/A\n",
      "      └─ Source: N/A → N/A\n",
      "   📤 Sending answer generation prompt to LLM...\n",
      "   📥 Answer generated (863 characters)\n",
      "\n",
      "🎉 PROCESSING COMPLETE\n",
      "   Final Context Quality: sufficient\n",
      "   Confidence: 0.80\n",
      "   Context Items Used: 5\n",
      "\n",
      "================================🎯 FINAL SUMMARY=================================\n",
      "Query: What are the best running shoes?\n",
      "Initial Context: 5 facts\n",
      "Final Context: 5 facts\n",
      "Quality: sufficient\n",
      "Confidence: 0.80\n",
      "Reasoning: Reasonable amount\n",
      "\n",
      "Detailed Context Used:\n",
      "  • Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole) is a type of Shoes....\n",
      "  • Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole) is a type of Shoes....\n",
      "  • TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) is a type of Shoes....\n",
      "  • TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) is a type of Shoes....\n",
      "  • TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole) has a size variant 7T....\n",
      "\n",
      "Final Answer:\n",
      "Based on the context provided, two types of running shoes mentioned are:\n",
      "\n",
      "1. **Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole)**: This shoe is designed for men and features a lightweight wool material, which can provide comfort and breathability during runs.\n",
      "\n",
      "2. **TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole)**: This shoe is specifically designed for little kids and also utilizes wool, which can be beneficial for warmth and comfort. It has a size variant of 7T, making it suitable for toddlers.\n",
      "\n",
      "While these options are highlighted, the best running shoes for you may depend on your specific needs, such as foot type, running style, and personal preferences. If you're looking for adult running shoes, the Men's SuperLight Wool Runners could be a great choice. For children, the TinyBirds Wool Runners would be appropriate.\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING QUERY: 'Tell me about customer service policies'\n",
      "================================================================================\n",
      "\n",
      "📝 STEP 1: INITIAL CONTEXT RETRIEVAL\n",
      "\n",
      "🔍 SEARCHING GRAPH DATABASE:\n",
      "   Query: 'Tell me about customer service policies'\n",
      "   Max Results: 5\n",
      "\n",
      "📊 GRAPH DATABASE RESULTS (0 items found):\n",
      "------------------------------------------------------------\n",
      "   ❌ No results found in graph database\n",
      "\n",
      "📝 STEP 2: INITIAL CONTEXT ASSESSMENT\n",
      "\n",
      "🔍 ASSESSING CONTEXT QUALITY:\n",
      "   Query: 'Tell me about customer service policies'\n",
      "   Context items: 0\n",
      "   📤 Sending assessment prompt to LLM...\n",
      "   ❌ Error in LLM assessment: Expecting value: line 1 column 1 (char 0)\n",
      "   🔄 Using fallback assessment: {'quality': 'insufficient', 'confidence': 1.0, 'reasoning': 'No context', 'can_answer': False, 'missing_info': 'No context available', 'relevance_score': 0.0}\n",
      "\n",
      "📝 STEP 3: CONTEXT OPTIMIZATION\n",
      "   🔄 Context insufficient - expanding search...\n",
      "\n",
      "🔍 SEARCHING GRAPH DATABASE:\n",
      "   Query: 'Tell me about customer service policies'\n",
      "   Max Results: 10\n",
      "\n",
      "📊 GRAPH DATABASE RESULTS (0 items found):\n",
      "------------------------------------------------------------\n",
      "   ❌ No results found in graph database\n",
      "\n",
      "🔧 OPTIMIZING CONTEXT:\n",
      "   Initial context items: 0\n",
      "   📈 No initial context - trying broader search...\n",
      "\n",
      "🔍 SEARCHING GRAPH DATABASE:\n",
      "   Query: 'Tell me about customer service policies'\n",
      "   Max Results: 10\n",
      "\n",
      "📊 GRAPH DATABASE RESULTS (0 items found):\n",
      "------------------------------------------------------------\n",
      "   ❌ No results found in graph database\n",
      "   ✅ Broader search returned 0 items, using first 5\n",
      "\n",
      "📝 STEP 4: FINAL CONTEXT ASSESSMENT\n",
      "\n",
      "🔍 ASSESSING CONTEXT QUALITY:\n",
      "   Query: 'Tell me about customer service policies'\n",
      "   Context items: 0\n",
      "   📤 Sending assessment prompt to LLM...\n",
      "   ❌ Error in LLM assessment: Expecting value: line 1 column 1 (char 0)\n",
      "   🔄 Using fallback assessment: {'quality': 'insufficient', 'confidence': 1.0, 'reasoning': 'No context', 'can_answer': False, 'missing_info': 'No context available', 'relevance_score': 0.0}\n",
      "\n",
      "📝 STEP 5: ANSWER GENERATION\n",
      "   ❌ Insufficient context - cannot provide reliable answer\n",
      "\n",
      "🎉 PROCESSING COMPLETE\n",
      "   Final Context Quality: insufficient\n",
      "   Confidence: 1.00\n",
      "   Context Items Used: 0\n",
      "\n",
      "================================🎯 FINAL SUMMARY=================================\n",
      "Query: Tell me about customer service policies\n",
      "Initial Context: 0 facts\n",
      "Final Context: 0 facts\n",
      "Quality: insufficient\n",
      "Confidence: 1.00\n",
      "Reasoning: No context\n",
      "\n",
      "Detailed Context Used:\n",
      "\n",
      "Final Answer:\n",
      "I don't have enough relevant information to answer your question properly.\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "🚀 PROCESSING QUERY: 'How do I return a product?'\n",
      "================================================================================\n",
      "\n",
      "📝 STEP 1: INITIAL CONTEXT RETRIEVAL\n",
      "\n",
      "🔍 SEARCHING GRAPH DATABASE:\n",
      "   Query: 'How do I return a product?'\n",
      "   Max Results: 5\n",
      "\n",
      "📊 GRAPH DATABASE RESULTS (5 items found):\n",
      "------------------------------------------------------------\n",
      " 1. Fact: Manybirds is the vendor of Men's Couriers - Natural Black/Basin Blue (Blizzard Sole).\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 2. Fact: jess is interested in buying a pair of shoes\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 3. Fact: Size 14 is available for Men's Couriers - Natural Black/Basin Blue (Blizzard Sole).\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 4. Fact: Size 12 is available for Men's Couriers - Natural Black/Basin Blue (Blizzard Sole).\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      " 5. Fact: jess asks what sizes the TinyBirds Wool Runners in Natural Black come in\n",
      "    Confidence: N/A\n",
      "    Source: N/A\n",
      "    Target: N/A\n",
      "    Type: N/A\n",
      "\n",
      "\n",
      "📝 STEP 2: INITIAL CONTEXT ASSESSMENT\n",
      "\n",
      "🔍 ASSESSING CONTEXT QUALITY:\n",
      "   Query: 'How do I return a product?'\n",
      "   Context items: 5\n",
      "   📤 Sending assessment prompt to LLM...\n",
      "   ❌ Error in LLM assessment: Expecting value: line 1 column 1 (char 0)\n",
      "   🔄 Using fallback assessment: {'quality': 'sufficient', 'confidence': 0.8, 'reasoning': 'Reasonable amount', 'can_answer': True, 'missing_info': 'N/A', 'relevance_score': 0.7}\n",
      "\n",
      "📝 STEP 3: CONTEXT OPTIMIZATION\n",
      "   ✅ Context quality sufficient - using as is\n",
      "\n",
      "📝 STEP 4: FINAL CONTEXT ASSESSMENT\n",
      "\n",
      "🔍 ASSESSING CONTEXT QUALITY:\n",
      "   Query: 'How do I return a product?'\n",
      "   Context items: 5\n",
      "   📤 Sending assessment prompt to LLM...\n",
      "   ❌ Error in LLM assessment: Expecting value: line 1 column 1 (char 0)\n",
      "   🔄 Using fallback assessment: {'quality': 'sufficient', 'confidence': 0.8, 'reasoning': 'Reasonable amount', 'can_answer': True, 'missing_info': 'N/A', 'relevance_score': 0.7}\n",
      "\n",
      "📝 STEP 5: ANSWER GENERATION\n",
      "   ✅ Sufficient context available - generating answer...\n",
      "\n",
      "📋 FINAL CONTEXT BEING USED FOR ANSWER:\n",
      "   1. Manybirds is the vendor of Men's Couriers - Natural Black/Basin Blue (Blizzard Sole).\n",
      "      └─ Confidence: N/A\n",
      "      └─ Source: N/A → N/A\n",
      "   2. jess is interested in buying a pair of shoes\n",
      "      └─ Confidence: N/A\n",
      "      └─ Source: N/A → N/A\n",
      "   3. Size 14 is available for Men's Couriers - Natural Black/Basin Blue (Blizzard Sole).\n",
      "      └─ Confidence: N/A\n",
      "      └─ Source: N/A → N/A\n",
      "   4. Size 12 is available for Men's Couriers - Natural Black/Basin Blue (Blizzard Sole).\n",
      "      └─ Confidence: N/A\n",
      "      └─ Source: N/A → N/A\n",
      "   5. jess asks what sizes the TinyBirds Wool Runners in Natural Black come in\n",
      "      └─ Confidence: N/A\n",
      "      └─ Source: N/A → N/A\n",
      "   📤 Sending answer generation prompt to LLM...\n",
      "   📥 Answer generated (761 characters)\n",
      "\n",
      "🎉 PROCESSING COMPLETE\n",
      "   Final Context Quality: sufficient\n",
      "   Confidence: 0.80\n",
      "   Context Items Used: 5\n",
      "\n",
      "================================🎯 FINAL SUMMARY=================================\n",
      "Query: How do I return a product?\n",
      "Initial Context: 5 facts\n",
      "Final Context: 5 facts\n",
      "Quality: sufficient\n",
      "Confidence: 0.80\n",
      "Reasoning: Reasonable amount\n",
      "\n",
      "Detailed Context Used:\n",
      "  • Manybirds is the vendor of Men's Couriers - Natural Black/Basin Blue (Blizzard Sole)....\n",
      "  • jess is interested in buying a pair of shoes...\n",
      "  • Size 14 is available for Men's Couriers - Natural Black/Basin Blue (Blizzard Sole)....\n",
      "  • Size 12 is available for Men's Couriers - Natural Black/Basin Blue (Blizzard Sole)....\n",
      "  • jess asks what sizes the TinyBirds Wool Runners in Natural Black come in...\n",
      "\n",
      "Final Answer:\n",
      "To return a product, you typically need to follow the vendor's return policy. Since Manybirds is the vendor for the Men's Couriers - Natural Black/Basin Blue (Blizzard Sole), you should check their website or contact their customer service for specific return instructions. \n",
      "\n",
      "If you purchased the Men's Couriers and need to return them, ensure you have your order details handy, such as the size (available sizes are 12 and 14) and the product name. If you are looking to return a different product, like the TinyBirds Wool Runners in Natural Black, you may need to inquire about their sizes and return policy as well, since that information wasn't provided in the context. \n",
      "\n",
      "For the best assistance, reach out directly to Manybirds with your order information.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "agent = SimpleContextAgent()\n",
    "\n",
    "# Test with different types of queries\n",
    "queries = [\n",
    "    \"What are the best running shoes?\",\n",
    "    \"Tell me about customer service policies\",\n",
    "    \"How do I return a product?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    result = await agent.answer_with_context(query)\n",
    "    \n",
    "    print(f\"\\n{'🎯 FINAL SUMMARY':=^80}\")\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Initial Context: {result['initial_context_count']} facts\")\n",
    "    print(f\"Final Context: {result['final_context_count']} facts\")\n",
    "    print(f\"Quality: {result['context_quality']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "    print(f\"Reasoning: {result['reasoning']}\")\n",
    "    print(f\"\\nDetailed Context Used:\")\n",
    "    for detail in result['detailed_context']:\n",
    "        print(f\"  • {detail['fact'][:100]}...\")\n",
    "    print(f\"\\nFinal Answer:\\n{result['answer']}\")\n",
    "    print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object demo at 0x0000022959465240>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

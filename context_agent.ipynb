{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from enum import Enum\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from graphiti_core import Graphiti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple context quality assessment\n",
    "class ContextQuality(Enum):\n",
    "    INSUFFICIENT = \"insufficient\"\n",
    "    SUFFICIENT = \"sufficient\"\n",
    "    EXCESSIVE = \"excessive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple context quality assessment\n",
    "class ContextQuality(Enum):\n",
    "    INSUFFICIENT = \"insufficient\"\n",
    "    SUFFICIENT = \"sufficient\"\n",
    "    EXCESSIVE = \"excessive\"\n",
    "\n",
    "class SimpleContextAgent:\n",
    "    def __init__(self):\n",
    "        # Setup\n",
    "        self.llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "        self.client = Graphiti(\n",
    "            os.environ.get('NEO4J_URI', 'bolt://localhost:7687'),\n",
    "            os.environ.get('NEO4J_USER', 'neo4j'),\n",
    "            os.environ.get('NEO4J_PASSWORD', 'password')\n",
    "        )\n",
    "        \n",
    "    async def get_context(self, query: str, max_results: int = 5) -> List[str]:\n",
    "        \"\"\"Get context from knowledge base\"\"\"\n",
    "        try:\n",
    "            results = await self.client.search(query, num_results=max_results)\n",
    "            return [edge.fact for edge in results]\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    async def assess_context_quality(self, query: str, context: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Check if context is sufficient, insufficient, or excessive\"\"\"\n",
    "        \n",
    "        assessment_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "        \n",
    "        Context Facts:\n",
    "        {chr(10).join([f\"- {fact}\" for fact in context]) if context else \"No context provided\"}\n",
    "        \n",
    "        Assess the context quality for answering this query. Respond in JSON:\n",
    "        {{\n",
    "            \"quality\": \"insufficient|sufficient|excessive\",\n",
    "            \"confidence\": 0.0-1.0,\n",
    "            \"reasoning\": \"brief explanation\",\n",
    "            \"can_answer\": true/false\n",
    "        }}\n",
    "        \n",
    "        Guidelines:\n",
    "        - insufficient: Missing key information needed to answer\n",
    "        - sufficient: Has enough relevant information to answer well\n",
    "        - excessive: Too much irrelevant information that clutters the response\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = await self.llm.ainvoke([HumanMessage(content=assessment_prompt)])\n",
    "            return json.loads(response.content)\n",
    "        except:\n",
    "            # Fallback simple assessment\n",
    "            if not context:\n",
    "                return {\"quality\": \"insufficient\", \"confidence\": 1.0, \"reasoning\": \"No context\", \"can_answer\": False}\n",
    "            elif len(context) > 10:\n",
    "                return {\"quality\": \"excessive\", \"confidence\": 0.7, \"reasoning\": \"Too many facts\", \"can_answer\": True}\n",
    "            else:\n",
    "                return {\"quality\": \"sufficient\", \"confidence\": 0.8, \"reasoning\": \"Reasonable amount\", \"can_answer\": True}\n",
    "    \n",
    "    async def optimize_context(self, query: str, initial_context: List[str]) -> List[str]:\n",
    "        \"\"\"Get the right amount of context - not too much, not too little\"\"\"\n",
    "        \n",
    "        if not initial_context:\n",
    "            # Try broader search\n",
    "            broader_context = await self.get_context(query, max_results=10)\n",
    "            return broader_context[:5]  # Cap at 5 for simplicity\n",
    "        \n",
    "        # Filter most relevant context\n",
    "        filter_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "        \n",
    "        Available Context:\n",
    "        {chr(10).join([f\"{i+1}. {fact}\" for i, fact in enumerate(initial_context)])}\n",
    "        \n",
    "        Select the 3-5 most relevant facts that directly help answer the query.\n",
    "        Return only the numbers (e.g., \"1,3,5\"):\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = await self.llm.ainvoke([HumanMessage(content=filter_prompt)])\n",
    "            selected_nums = [int(n.strip()) for n in response.content.split(',') if n.strip().isdigit()]\n",
    "            return [initial_context[i-1] for i in selected_nums if 0 < i <= len(initial_context)]\n",
    "        except:\n",
    "            return initial_context[:5]  # Fallback to first 5\n",
    "    \n",
    "    async def answer_with_context(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main method: Get context, assess quality, and answer\"\"\"\n",
    "        \n",
    "        # Step 1: Get initial context\n",
    "        initial_context = await self.get_context(query)\n",
    "        \n",
    "        # Step 2: Assess context quality\n",
    "        assessment = await self.assess_context_quality(query, initial_context)\n",
    "        \n",
    "        # Step 3: Optimize context if needed\n",
    "        if assessment[\"quality\"] == \"insufficient\":\n",
    "            # Try to get more context\n",
    "            expanded_context = await self.get_context(query, max_results=10)\n",
    "            optimized_context = await self.optimize_context(query, expanded_context)\n",
    "        elif assessment[\"quality\"] == \"excessive\":\n",
    "            # Reduce context to most relevant\n",
    "            optimized_context = await self.optimize_context(query, initial_context)\n",
    "        else:\n",
    "            optimized_context = initial_context\n",
    "        \n",
    "        # Step 4: Final assessment\n",
    "        final_assessment = await self.assess_context_quality(query, optimized_context)\n",
    "        \n",
    "        # Step 5: Generate answer\n",
    "        if final_assessment[\"can_answer\"]:\n",
    "            answer_prompt = f\"\"\"\n",
    "            Query: {query}\n",
    "            \n",
    "            Context:\n",
    "            {chr(10).join([f\"- {fact}\" for fact in optimized_context])}\n",
    "            \n",
    "            Provide a helpful answer based on the context above.\n",
    "            \"\"\"\n",
    "            \n",
    "            answer_response = await self.llm.ainvoke([HumanMessage(content=answer_prompt)])\n",
    "            answer = answer_response.content\n",
    "        else:\n",
    "            answer = \"I don't have enough relevant information to answer your question properly.\"\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"initial_context_count\": len(initial_context),\n",
    "            \"final_context_count\": len(optimized_context),\n",
    "            \"context_quality\": final_assessment[\"quality\"],\n",
    "            \"confidence\": final_assessment[\"confidence\"],\n",
    "            \"reasoning\": final_assessment[\"reasoning\"],\n",
    "            \"context_facts\": optimized_context,\n",
    "            \"answer\": answer\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool for external use\n",
    "@tool\n",
    "async def smart_search(query: str) -> str:\n",
    "    \"\"\"Search with intelligent context optimization\"\"\"\n",
    "    agent = SimpleContextAgent()\n",
    "    result = await agent.answer_with_context(query)\n",
    "    \n",
    "    return f\"\"\"\n",
    "    Context Quality: {result['context_quality']} (confidence: {result['confidence']:.2f})\n",
    "    Context Used: {result['final_context_count']} facts\n",
    "    Reasoning: {result['reasoning']}\n",
    "    \n",
    "    Answer: {result['answer']}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Query: What are the best running shoes?\n",
      "==================================================\n",
      "Initial Context: 5 facts\n",
      "Final Context: 5 facts\n",
      "Quality: sufficient\n",
      "Confidence: 0.80\n",
      "Reasoning: Reasonable amount\n",
      "Answer: Based on the context provided, the best running shoes mentioned include:\n",
      "\n",
      "1. **Men's SuperLight Wool Runners - Dark Grey (Medium Grey Sole)**: These shoes are designed for men and are made from lightweight wool, which can provide comfort and breathability during runs.\n",
      "\n",
      "2. **TinyBirds Wool Runners - Little Kids - Natural Black (Blizzard Sole)**: These shoes are suitable for little kids and also made from wool, offering a cozy fit for young runners. They come in a size variant of 7T.\n",
      "\n",
      "While the context does not provide a comprehensive list of all the best running shoes available, these options highlight some quality choices for both men and children. When selecting running shoes, consider factors such as fit, comfort, and the specific needs of your running style.\n",
      "\n",
      "==================================================\n",
      "Query: Tell me about customer service policies\n",
      "==================================================\n",
      "Initial Context: 0 facts\n",
      "Final Context: 0 facts\n",
      "Quality: insufficient\n",
      "Confidence: 1.00\n",
      "Reasoning: No context\n",
      "Answer: I don't have enough relevant information to answer your question properly.\n",
      "\n",
      "==================================================\n",
      "Query: How do I return a product?\n",
      "==================================================\n",
      "Initial Context: 5 facts\n",
      "Final Context: 5 facts\n",
      "Quality: sufficient\n",
      "Confidence: 0.80\n",
      "Reasoning: Reasonable amount\n",
      "Answer: To return a product, you typically need to follow the vendor's return policy. Since Manybirds is the vendor for the Men's Couriers - Natural Black/Basin Blue (Blizzard Sole), you should check their website or contact their customer service for specific return instructions. Generally, you may need to provide your order number, the reason for the return, and ensure the product is in its original condition. If you have any further questions or need assistance, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "agent = SimpleContextAgent()\n",
    "\n",
    "# Test with different types of queries\n",
    "queries = [\n",
    "    \"What are the best running shoes?\",\n",
    "    \"Tell me about customer service policies\",\n",
    "    \"How do I return a product?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    result = await agent.answer_with_context(query)\n",
    "    \n",
    "    print(f\"Initial Context: {result['initial_context_count']} facts\")\n",
    "    print(f\"Final Context: {result['final_context_count']} facts\")\n",
    "    print(f\"Quality: {result['context_quality']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2f}\")\n",
    "    print(f\"Reasoning: {result['reasoning']}\")\n",
    "    print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object demo at 0x0000022959465240>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
